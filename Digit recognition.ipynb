{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are very effective in image classification. If we simply add one layer of conv filters to our fully connected NN (before the dense layers), we'll beat 99%. For example in Keras: model.add(Conv2D(filters=32,kernel_size=5,activation='relu')) The simple network 784-32C5-500-10 with dropout achieves 99% after 30 epochs. (Add a pooling layer and it gets there in 15 epochs.)\n",
    "\n",
    "**Best Design:** The best design is to start with convolution layer(s), then add a pooling layer. Then add convolution layer(s), then another pooling. Then add dense layer(s). Finish with softmax. Here's a simple schematic (from LeNet5): \n",
    "\n",
    "![LeNet5 architecture](images/LaNet5-architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of **Kaggle leaderboard scores:**\n",
    "\n",
    "![Histogram of Kaggle leaderboard scores](images/KaggleMNISThist3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Layer Deep CNN Model with Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 223,850\n",
      "Trainable params: 223,402\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 14s 48ms/step - loss: 1.1971 - acc: 0.5931 - val_loss: 0.1868 - val_acc: 0.9417\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 12s 38ms/step - loss: 0.4644 - acc: 0.8611 - val_loss: 0.1364 - val_acc: 0.9583\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.3435 - acc: 0.9027 - val_loss: 0.0759 - val_acc: 0.9786\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.2844 - acc: 0.9173 - val_loss: 0.0723 - val_acc: 0.9795\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.2161 - acc: 0.9356 - val_loss: 0.0633 - val_acc: 0.9812\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.2120 - acc: 0.9395 - val_loss: 0.0642 - val_acc: 0.9831\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.1796 - acc: 0.9470 - val_loss: 0.0902 - val_acc: 0.9740\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 0.1808 - acc: 0.9517 - val_loss: 0.0517 - val_acc: 0.9848\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1448 - acc: 0.9584 - val_loss: 0.0572 - val_acc: 0.9845\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1679 - acc: 0.9545 - val_loss: 0.0481 - val_acc: 0.9869\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1511 - acc: 0.9605 - val_loss: 0.0513 - val_acc: 0.9860\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1347 - acc: 0.9636 - val_loss: 0.0479 - val_acc: 0.9867\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1215 - acc: 0.9655 - val_loss: 0.0443 - val_acc: 0.9864\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.1368 - acc: 0.9603 - val_loss: 0.0482 - val_acc: 0.9876\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1257 - acc: 0.9648 - val_loss: 0.0392 - val_acc: 0.9898\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1048 - acc: 0.9716 - val_loss: 0.0379 - val_acc: 0.9886\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 13s 42ms/step - loss: 0.1082 - acc: 0.9722 - val_loss: 0.0435 - val_acc: 0.9888\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1029 - acc: 0.9709 - val_loss: 0.0391 - val_acc: 0.9890\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1011 - acc: 0.9712 - val_loss: 0.0306 - val_acc: 0.9917\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 0.1041 - acc: 0.9704 - val_loss: 0.0348 - val_acc: 0.9905\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1034 - acc: 0.9714 - val_loss: 0.0385 - val_acc: 0.9876\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0966 - acc: 0.9741 - val_loss: 0.0296 - val_acc: 0.9907\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.0838 - acc: 0.9771 - val_loss: 0.0336 - val_acc: 0.9917\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 0.0874 - acc: 0.9757 - val_loss: 0.0324 - val_acc: 0.9917\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0818 - acc: 0.9772 - val_loss: 0.0298 - val_acc: 0.9921\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0842 - acc: 0.9767 - val_loss: 0.0296 - val_acc: 0.9907\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0814 - acc: 0.9777 - val_loss: 0.0288 - val_acc: 0.9919\n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0759 - acc: 0.9797 - val_loss: 0.0344 - val_acc: 0.9907\n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0789 - acc: 0.9778 - val_loss: 0.0310 - val_acc: 0.9910\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.0745 - acc: 0.9799 - val_loss: 0.0244 - val_acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "Y_train = train_data[\"label\"]\n",
    "X_train = train_data.drop(labels = [\"label\"],axis = 1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = test_data / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=5,input_shape=(28, 28, 1), activation = 'relu'))\n",
    "model.add(Conv2D(32, kernel_size=5, activation = 'relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer=Adam(lr=0.001)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "model_try = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n",
    "                              epochs = 30, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=300, callbacks=[annealer])\n",
    "                              \n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions,axis = 1)\n",
    "predictions = pd.Series(predictions, name=\"Label\")\n",
    "submit = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),predictions],axis = 1)\n",
    "submit.to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(model, open(\"digits_deeplearn.pkl\", \"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "from keras.models import load_model \n",
    "model.save('digits_deeplearn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.36%\n",
      "CNN Error: 0.64%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Training Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02143886872308247, 0.9945238095238095]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to directly load the model from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# model = pickle.load(open(\"digits_deeplearn.pkl\",\"rb\"))\n",
    "from keras.models import load_model \n",
    "model = load_model('digits_deeplearn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the image obtained from HTML canvas and predicting the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The input is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert(\"RGB\")\n",
    "    im = im.convert(\"L\")\n",
    "\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    new_image = Image.new('L', (28, 28), 255)  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if nheight == 0:  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        new_image.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Height becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if nwidth == 0:  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        new_image.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(new_image.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "    return tva\n",
    "\n",
    "\n",
    "def remove_transparent(path):\n",
    "    im = Image.open(path).convert(\"RGBA\")\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    canvas = Image.new('RGBA', im.size, (255, 255, 255, 255))  # Empty canvas colour (r,g,b,a)\n",
    "    canvas.paste(im, mask=im)  # Paste the image onto the canvas, using it's alpha channel as mask\n",
    "    canvas.thumbnail([width, height], Image.ANTIALIAS)\n",
    "    canvas.save(path, format=\"PNG\")\n",
    "\n",
    "def convert_image(image_path):\n",
    "    remove_transparent(image_path)\n",
    "    x = [imageprepare(image_path)]  # file path here\n",
    "    # Now we convert 784 sized 1d array to 24x24 sized 2d array so that we can visualize it\n",
    "    new_arr = [[0 for d in range(28)] for y in range(28)]\n",
    "    k = 0\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            new_arr[i][j] = x[0][k]\n",
    "            k = k + 1\n",
    "\n",
    "    return new_arr\n",
    "\n",
    "new_img=convert_image('C:/xampp/htdocs/digit_recog/tmp/img.png')\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(new_img, cmap=plt.cm.gray_r, interpolation='kaiser')\n",
    "npimg=np.array(new_img)\n",
    "npimg[npimg>0.1]=1\n",
    "npimg[npimg<=0.1]=0\n",
    "npimg=npimg.reshape(-1,28,28,1)\n",
    "predicted = model.predict(npimg)\n",
    "predicted = np.argmax(predicted,axis = 1)\n",
    "print(\"Predicted: \",predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=model.predict_proba(npimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence:  99.96141195297241\n"
     ]
    }
   ],
   "source": [
    "print(\"Confidence: \",predicted[0][3]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Confusion matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 978,    0,    0,    0,    0,    0,    2,    0,    0,    0],\n",
       "       [   0, 1129,    0,    1,    0,    0,    1,    3,    1,    0],\n",
       "       [   1,    1, 1026,    2,    0,    0,    0,    2,    0,    0],\n",
       "       [   0,    0,    0, 1009,    0,    1,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,  979,    0,    0,    0,    1,    2],\n",
       "       [   0,    0,    0,    6,    0,  884,    1,    1,    0,    0],\n",
       "       [   3,    2,    0,    0,    1,    0,  950,    0,    2,    0],\n",
       "       [   0,    2,    1,    1,    0,    0,    0, 1023,    0,    1],\n",
       "       [   0,    0,    0,    1,    0,    1,    0,    0,  971,    1],\n",
       "       [   0,    0,    0,    0,    2,    1,    0,    0,    0, 1006]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_test = X_test/255.0\n",
    "\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "y_pred_ohe = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_labels)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
