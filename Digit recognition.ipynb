{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are very effective in image classification. If we simply add one layer of conv filters to our fully connected NN (before the dense layers), we'll beat 99%. For example in Keras: model.add(Conv2D(filters=32,kernel_size=5,activation='relu')) The simple network 784-32C5-500-10 with dropout achieves 99% after 30 epochs. (Add a pooling layer and it gets there in 15 epochs.)\n",
    "\n",
    "**Best Design:** The best design is to start with convolution layer(s), then add a pooling layer. Then add convolution layer(s), then another pooling. Then add dense layer(s). Finish with softmax. Here's a simple schematic (from LeNet5): \n",
    "\n",
    "![LeNet5 architecture](images/LaNet5-architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of **Kaggle leaderboard scores:**\n",
    "\n",
    "![Histogram of Kaggle leaderboard scores](images/KaggleMNISThist3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Layer Deep CNN Model with Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 223,850\n",
      "Trainable params: 223,402\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 14s 48ms/step - loss: 1.1971 - acc: 0.5931 - val_loss: 0.1868 - val_acc: 0.9417\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 12s 38ms/step - loss: 0.4644 - acc: 0.8611 - val_loss: 0.1364 - val_acc: 0.9583\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.3435 - acc: 0.9027 - val_loss: 0.0759 - val_acc: 0.9786\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.2844 - acc: 0.9173 - val_loss: 0.0723 - val_acc: 0.9795\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.2161 - acc: 0.9356 - val_loss: 0.0633 - val_acc: 0.9812\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.2120 - acc: 0.9395 - val_loss: 0.0642 - val_acc: 0.9831\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.1796 - acc: 0.9470 - val_loss: 0.0902 - val_acc: 0.9740\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 0.1808 - acc: 0.9517 - val_loss: 0.0517 - val_acc: 0.9848\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1448 - acc: 0.9584 - val_loss: 0.0572 - val_acc: 0.9845\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1679 - acc: 0.9545 - val_loss: 0.0481 - val_acc: 0.9869\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1511 - acc: 0.9605 - val_loss: 0.0513 - val_acc: 0.9860\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1347 - acc: 0.9636 - val_loss: 0.0479 - val_acc: 0.9867\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.1215 - acc: 0.9655 - val_loss: 0.0443 - val_acc: 0.9864\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 0.1368 - acc: 0.9603 - val_loss: 0.0482 - val_acc: 0.9876\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1257 - acc: 0.9648 - val_loss: 0.0392 - val_acc: 0.9898\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1048 - acc: 0.9716 - val_loss: 0.0379 - val_acc: 0.9886\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 13s 42ms/step - loss: 0.1082 - acc: 0.9722 - val_loss: 0.0435 - val_acc: 0.9888\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1029 - acc: 0.9709 - val_loss: 0.0391 - val_acc: 0.9890\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1011 - acc: 0.9712 - val_loss: 0.0306 - val_acc: 0.9917\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 12s 41ms/step - loss: 0.1041 - acc: 0.9704 - val_loss: 0.0348 - val_acc: 0.9905\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.1034 - acc: 0.9714 - val_loss: 0.0385 - val_acc: 0.9876\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0966 - acc: 0.9741 - val_loss: 0.0296 - val_acc: 0.9907\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.0838 - acc: 0.9771 - val_loss: 0.0336 - val_acc: 0.9917\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 0.0874 - acc: 0.9757 - val_loss: 0.0324 - val_acc: 0.9917\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0818 - acc: 0.9772 - val_loss: 0.0298 - val_acc: 0.9921\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0842 - acc: 0.9767 - val_loss: 0.0296 - val_acc: 0.9907\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0814 - acc: 0.9777 - val_loss: 0.0288 - val_acc: 0.9919\n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 11s 38ms/step - loss: 0.0759 - acc: 0.9797 - val_loss: 0.0344 - val_acc: 0.9907\n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 0.0789 - acc: 0.9778 - val_loss: 0.0310 - val_acc: 0.9910\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 12s 39ms/step - loss: 0.0745 - acc: 0.9799 - val_loss: 0.0244 - val_acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "Y_train = train_data[\"label\"]\n",
    "X_train = train_data.drop(labels = [\"label\"],axis = 1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = test_data / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=5,input_shape=(28, 28, 1), activation = 'relu'))\n",
    "model.add(Conv2D(32, kernel_size=5, activation = 'relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer=Adam(lr=0.001)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "model_try = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n",
    "                              epochs = 30, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=300, callbacks=[annealer])\n",
    "                              \n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions,axis = 1)\n",
    "predictions = pd.Series(predictions, name=\"Label\")\n",
    "submit = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),predictions],axis = 1)\n",
    "submit.to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(model, open(\"digits_deeplearn.pkl\", \"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "from keras.models import load_model \n",
    "model.save('digits_deeplearn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.36%\n",
      "CNN Error: 0.64%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Training Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02143886872308247, 0.9945238095238095]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to directly load the model from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# model = pickle.load(open(\"digits_deeplearn.pkl\",\"rb\"))\n",
    "from keras.models import load_model \n",
    "model = load_model('digits_deeplearn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the image obtained from HTML canvas and predicting the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The input is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert(\"RGB\")\n",
    "    im = im.convert(\"L\")\n",
    "\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    new_image = Image.new('L', (28, 28), 255)  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if nheight == 0:  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        new_image.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Height becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if nwidth == 0:  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        new_image.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(new_image.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "    return tva\n",
    "\n",
    "\n",
    "def remove_transparent(path):\n",
    "    im = Image.open(path).convert(\"RGBA\")\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    canvas = Image.new('RGBA', im.size, (255, 255, 255, 255))  # Empty canvas colour (r,g,b,a)\n",
    "    canvas.paste(im, mask=im)  # Paste the image onto the canvas, using it's alpha channel as mask\n",
    "    canvas.thumbnail([width, height], Image.ANTIALIAS)\n",
    "    canvas.save(path, format=\"PNG\")\n",
    "\n",
    "def convert_image(image_path):\n",
    "    remove_transparent(image_path)\n",
    "    x = [imageprepare(image_path)]  # file path here\n",
    "    # Now we convert 784 sized 1d array to 24x24 sized 2d array so that we can visualize it\n",
    "    new_arr = [[0 for d in range(28)] for y in range(28)]\n",
    "    k = 0\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            new_arr[i][j] = x[0][k]\n",
    "            k = k + 1\n",
    "\n",
    "    return new_arr\n",
    "\n",
    "new_img=convert_image('C:/xampp/htdocs/digit_recog/tmp/img.png')\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(new_img, cmap=plt.cm.gray_r, interpolation='kaiser')\n",
    "npimg=np.array(new_img)\n",
    "npimg[npimg>0.1]=1\n",
    "npimg[npimg<=0.1]=0\n",
    "npimg=npimg.reshape(-1,28,28,1)\n",
    "predicted = model.predict(npimg)\n",
    "predicted = np.argmax(predicted,axis = 1)\n",
    "print(\"Predicted: \",predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Confusion matrix for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sklearn.metrics as metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_test = X_test/255.0\n",
    "\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "y_pred_ohe = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_labels)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
